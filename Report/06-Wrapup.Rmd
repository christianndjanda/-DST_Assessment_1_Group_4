---
title: "Wrapup"
author: "Katherine Whitehead"
date: "2024-10-26"
output: html_document
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Conclusions

Each of the 4 base models performed quite well once they had been tuned
and the data transformed in an optimal way. The performances for each of
these models are seen below:

```{r}
library(knitr) 
data = data.frame( 
Model = c("KNN", "Logistic Regression", "Decision Tree","SVM"), 
Precision_1 = c('94%', '96%','92%','93%'),
Recall_1 = c('92%', '88%', '94%','94%'),
Accuracy =c('89%', '88%', '89%','90%')
)
kable(data, format = "markdown")
```

We mainly focus on the precision of the model, as in the context of
identifying loans that will be paid back we only want to invest in those
that can guarantee our money will not be lost. Hence we have a high
level of importance resting on a high true positive rate and a very low
false positive rate. In fact, in an ideal world we would want a 0 false
positive rate and would probably sacrifice a higher true positive rate in
order to achieve this.

Models were then stacked, and the same performance metrics were measured.
We see them in the table below, alongside the other models:

```{r}
library(knitr) 
data = data.frame( 
Model = c("KNN", "Logistic Regression", "Decision Tree","SVM",'Stacked'), 
Precision_1 = c('94%', '96%','92%','93%','93%'),
Recall_1 = c('92%', '88%', '94%','94%','95%'),
Accuracy =c('89%', '88%', '89%','90%','90%')
)
kable(data, format = "markdown")
```

As we see from the overall results across all the models, logistic
regression has by far the highest precision, however it also has the
lowest recall which means that is is missing out in more potential good
investments. Looking at the stacked model compared to all the other
models, we see that its performance has not change much. It follows very
closely the performance of the SVM and decision tree model. This then
poses the question as to whether it is worth the extra cost of stacking
the models in order to just get very similar results as before.
